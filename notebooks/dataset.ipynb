{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_folder(read_folder, write_folder):\n",
    "    '''\n",
    "    Read folder is a folder containing files in the format: \n",
    "    <artricle title>\\n\\n<article body>\\n\\n<article highlights>\\n\\n<entity map>\n",
    "    \n",
    "    parses each file into body, output and highlights into\n",
    "    write_folder\\\n",
    "        documents\\      # contains article body\n",
    "        summaries\\      # contains article highlight\n",
    "        outputs\\        # contains binary labels of sentences\n",
    "        \n",
    "    TODO: parsed files end with `\\n` which adds an empty line while converting to indices,\n",
    "        make sure parsed files don't end with new line\n",
    "    '''\n",
    "    \n",
    "    doc_folder = os.path.join(write_folder, \"documents\")\n",
    "    summary_folder = os.path.join(write_folder, \"summaries\")\n",
    "    output_folder = os.path.join(write_folder, \"outputs\")\n",
    "    \n",
    "    if not os.path.exists(doc_folder): os.mkdir(doc_folder)\n",
    "    if not os.path.exists(summary_folder): os.mkdir(summary_folder)\n",
    "    if not os.path.exists(output_folder): os.mkdir(output_folder)\n",
    "\n",
    "    i = 0\n",
    "    files = sorted(os.listdir(read_folder))\n",
    "    for filename in files:\n",
    "        try:\n",
    "            print(i, filename, end='\\r')\n",
    "            content = open(os.path.join(read_folder, filename)).read()\n",
    "            parts = content.split('\\n\\n')\n",
    "            lines = parts[1].split('\\n')\n",
    "            output = [int(line[-1]) for line in lines]\n",
    "            lines = [line.split('\\t')[0] for line in lines]\n",
    "            summary = parts[2].split('\\n')\n",
    "            entities = parts[3].split('\\n')\n",
    "\n",
    "            entity_map = {}\n",
    "            new_lines = []\n",
    "            for line in entities:\n",
    "                try:\n",
    "                    id, name = line.split(\":\")\n",
    "                    entity_map[id] = name\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for line in lines:\n",
    "                words = line.split(' ')\n",
    "                new_words = [entity_map[word] if word in entity_map else word for word in words]\n",
    "                line = \"\"\n",
    "                for word in new_words:\n",
    "                    line += word + \" \"\n",
    "                line += '\\n'\n",
    "                new_lines.append(line)\n",
    "            with open(os.path.join(doc_folder, str(i)), 'w+') as file:\n",
    "                file.writelines(new_lines)\n",
    "\n",
    "            new_summary = []\n",
    "            for line in summary:\n",
    "                words = line.split(' ')\n",
    "                new_words = [entity_map[word] if word in entity_map else word for word in words]\n",
    "                line = \"\"\n",
    "                for word in new_words:\n",
    "                    line += word + \" \"\n",
    "                line += '\\n'\n",
    "                new_summary.append(line)\n",
    "            with open(os.path.join(summary_folder, str(i)), 'w+') as file:\n",
    "                file.writelines(new_summary)\n",
    "                \n",
    "            with open(os.path.join(output_folder, str(i)), 'w+') as file:\n",
    "                for val in output:\n",
    "                    file.write(str(val) + '\\n')\n",
    "\n",
    "            i += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all(root_folder, write_folder):\n",
    "    '''\n",
    "    calls parse_folder function for each subfolder in the root_folder\n",
    "    and creates corresponding folders in write_folder\n",
    "    '''\n",
    "    if not os.path.exists(write_folder): os.makedirs(write_folder)\n",
    "    folders = os.listdir(root_folder)\n",
    "    folders = [folder for folder in folders if os.path.isdir(os.path.join(root_folder, folder))]\n",
    "    for folder in folders:\n",
    "        dest_folder = os.path.join(write_folder, folder)\n",
    "        if os.path.exists(dest_folder): shutil.rmtree(dest_folder)\n",
    "        os.mkdir(dest_folder)\n",
    "        parse_folder(os.path.join(root_folder, folder), dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSumToEmbedding(GloveEmbeddings):\n",
    "    '''\n",
    "    Class which handles conversion of text to indices\n",
    "    Inherits from GloveEmbeddings class in embeddings.ipynb\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, glove_filename=None, dump_filename=None, extra_vocab_filename=None):\n",
    "        super(NeuralSumToEmbedding, self).__init__()\n",
    "        if glove_filename:\n",
    "            self.load_glove(glove_filename)\n",
    "        elif dump_filename:\n",
    "            self.load_dump(dump_filename)\n",
    "        if extra_vocab_filename:\n",
    "            extra_vocab = pickle.load(open(extra_vocab_filename, 'rb'))[1]\n",
    "            for i in extra_vocab.keys():\n",
    "                self.add_to_vocab(i)\n",
    "        self.vectors.reshape(len(self.words), -1)\n",
    "        \n",
    "    def tokeniser(self, text):\n",
    "        '''\n",
    "        text is \\n seperated lines and <space> seperated tokens\n",
    "        '''\n",
    "        lines = text.split('\\n')\n",
    "        words = [line.split() for line in lines]\n",
    "        return words\n",
    "    \n",
    "    def convert_to_indices_(self, document_folder, dump_file=None):\n",
    "        '''\n",
    "        converts all files in documents to indices and pickles them into dump_file\n",
    "        TODO: if dump_file is None function won't work\n",
    "        '''\n",
    "        filenames = sorted(os.listdir(document_folder), key=lambda x: int(x))\n",
    "        indices = []\n",
    "        for filename in filenames:\n",
    "            with open(os.path.join(document_folder, filename)) as docfile:\n",
    "                text = self.tokeniser(docfile.read())\n",
    "                indices_ = self.convert_to_indices(text)\n",
    "                indices.append(indices_)\n",
    "     \n",
    "            print(filename, end='\\r')\n",
    "            \n",
    "        if dump_file:\n",
    "            pickle.dump(indices, open(dump_file, 'wb+'))\n",
    "            \n",
    "    def root_convert(self, root_read, root_dest):\n",
    "        '''\n",
    "        calls convert_to_indices_ to all subfolders of root_read and \n",
    "        creates corresponding files in root_dest\n",
    "        '''\n",
    "        folders = [folder for folder in os.listdir(root_read) if os.path.isdir(os.path.join(root_read, folder))]\n",
    "        if os.path.exists(root_dest): shutil.rmtree(root_dest)\n",
    "        os.makedirs(root_dest)\n",
    "        for folder in folders:\n",
    "            dest_file = os.path.join(root_dest, folder + '.pkl')\n",
    "            document_folder = os.path.join(root_read, folder, \"documents\")\n",
    "            self.convert_to_indices_(document_folder, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_dump(self, filename):\n",
    "        self.data = pickle.load(open(filename, 'rb'))\n",
    "        \n",
    "    def pad(self, max_sen_len=50, max_doc_len=90, output=False, padding_idx=400001):\n",
    "        '''\n",
    "        pades zeros to the right of each sentence upto a maximum length.\n",
    "        TODO: masks? left padding?\n",
    "        '''\n",
    "        self.lines = []\n",
    "        for doc in self.data:\n",
    "            self.lines.append([])\n",
    "            doc = doc[:max_doc_len]\n",
    "            for line in doc:\n",
    "                self.lines[-1].append([padding_idx for _ in range(max_sen_len - len(line))] + line[:max_sen_len])\n",
    "                \n",
    "    def padded_dump(self, filename):\n",
    "        pickle.dump([self.lines, self.output], open(filename, 'wb+'))\n",
    "        \n",
    "    def load_padded_dump(self, filename, truncate_sum=5):\n",
    "        self.lines  = pickle.load(open(filename, 'rb'))\n",
    "        \n",
    "    def extend_padded_dump(self, filename, truncate_sum=15):\n",
    "        lines = pickle.load(open(filename, 'rb'))   \n",
    "        self.lines.extend(lines)        \n",
    "        \n",
    "    def make_batches(self, batch_size):\n",
    "        '''\n",
    "        creates batches by grouping indices of same length together.\n",
    "        TODO: make documents of different length to be in same batch\n",
    "        '''\n",
    "        self.lengths = [[] for _ in range(90)]\n",
    "        self.batches = []\n",
    "        for i in range(len(self.data)):\n",
    "            self.lengths[len(self.lines[i]) -1].append(i)\n",
    "        \n",
    "        for i in self.lengths:\n",
    "            random.shuffle(i)\n",
    "            \n",
    "        for i in self.lengths:\n",
    "            for j in range(0, len(i), batch_size):\n",
    "                self.batches.append(i[j:j+batch_size])\n",
    "        random.shuffle(self.batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
