{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parseNeuralSum():\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.entity_map = {}\n",
    "        \n",
    "    def parse_file(self, filename):\n",
    "        content = open(filename).read()\n",
    "        parts = content.split('\\n\\n')\n",
    "        lines = parts[1].split('\\n')\n",
    "        output = [int(line[-1]) for line in lines]\n",
    "        lines = [line.split('\\t')[0] for line in lines]\n",
    "        summary = parts[2].split('\\n')\n",
    "        return lines, output, summary\n",
    "    \n",
    "    def parse_folder_1(self, input_folder, target_document_folder, target_output_folder=None, \n",
    "                     target_summary_folder=None):\n",
    "        filenames = os.listdir(input_folder)\n",
    "        id = 0\n",
    "        for filename in filenames:\n",
    "            lines, output, summary = self.parse_file(input_folder + filename)\n",
    "            with open(target_document_folder + str(id), 'w+') as file:\n",
    "                [file.write(line + '\\n') for line in lines]\n",
    "            if target_output_folder:\n",
    "                with open(target_output_folder + str(id), 'w+') as file:\n",
    "                    for num in output:\n",
    "                        file.write(str(num) + '\\n')\n",
    "            if target_summary_folder:\n",
    "                with open(target_summary_folder + str(id), 'w+') as file:\n",
    "                    [file.write(line + '\\n') for line in summary]\n",
    "            id += 1\n",
    "            print(id, end='\\r')\n",
    "        \n",
    "    def dump_all(self, filename):\n",
    "        pickle.dump([self.vocab, self.entity_map], open(filename, 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloveEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.word2id = {}\n",
    "        self.vectors = []\n",
    "        self.words = []\n",
    "        self.dim = None\n",
    "        \n",
    "    def load_glove(self, filename):\n",
    "        id = 0\n",
    "        with open(filename) as file:\n",
    "            for line_ in file:\n",
    "                line = line_.split()\n",
    "                word = line[0]\n",
    "                self.words.append(word)\n",
    "                self.word2id[word] , id = id, id + 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                self.vectors.append(vect)\n",
    "        self.vectors = np.array(self.vectors)\n",
    "        self.dim = self.vectors.shape[-1]\n",
    "        self.add_to_vocab('<unk>')\n",
    "    \n",
    "    def modify_pretrained(self, vocab):\n",
    "        pass\n",
    "    \n",
    "    def add_to_vocab(self, word):\n",
    "        word = word.lower()\n",
    "        self.words.append(word)\n",
    "        self.word2id[word] = len(self.words) - 1\n",
    "        self.vectors = np.append(self.vectors, np.random.random(self.dim))\n",
    "        \n",
    "    def dump_all(self, filename):\n",
    "        pickle.dump([self.word2id, self.vectors.reshape(-1), self.words, self.dim], open(filename, 'wb+'))\n",
    "        \n",
    "    def load_dump(self, filename):\n",
    "        self.word2id, self.vectors, self.words, self.dim = pickle.load(open(filename, 'rb+'))\n",
    "        self.vectors = self.vectors.reshape(-1, self.dim)\n",
    "        \n",
    "    def convert_to_indices(self, lines):\n",
    "        indices = []\n",
    "        for line in lines:\n",
    "            indices.append([])\n",
    "            for word in line:\n",
    "                if word in self.word2id:\n",
    "                    id = self.word2id[word]\n",
    "                else:\n",
    "                    id = self.word2id['<unk>']\n",
    "                indices[-1].append(id)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralSumGlove(GloveEmbeddings):\n",
    "    def __init__(self, glove_filename=None, dump_filename=None):\n",
    "        super(NeuralSumGlove, self).__init__()\n",
    "        if glove_filename:\n",
    "            self.load_glove(glove_filename)\n",
    "        else:\n",
    "            self.load_dump(dump_filename)\n",
    "        \n",
    "    def tokeniser(self, text):\n",
    "        lines = text.split('\\n')[:-1]\n",
    "        words = [line.split() for line in lines]\n",
    "        return words\n",
    "    \n",
    "    def convert_to_indices_(self, document_folder, output_folder=None, dump_file=None):\n",
    "        filenames = sorted(os.listdir(document_folder), key=lambda x: int(x))\n",
    "        indices = []\n",
    "        for filename in filenames:\n",
    "            indices.append([])\n",
    "            with open(document_folder + filename) as docfile:\n",
    "                text = self.tokeniser(docfile.read())\n",
    "                indices_ = self.convert_to_indices(text)\n",
    "                indices[-1].append(indices_)\n",
    "            if output_folder:\n",
    "                with open(output_folder + filename) as outputfile:\n",
    "                    text = outputfile.read().split('\\n')[:-1]\n",
    "                    output = [int(line) for line in text]\n",
    "                    indices[-1].append(output)\n",
    "            print(filename, end='\\r')\n",
    "        if dump_file:\n",
    "            pickle.dump(indices, open(dump_file, 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralSumDataHandler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_dump(self, filename):\n",
    "        self.data = pickle.load(open(filename, 'rb'))\n",
    "        \n",
    "    def pad(self, max_sen_len=50, max_doc_len=90, output=False):\n",
    "        self.lines = []\n",
    "        self.output = []\n",
    "        if output:\n",
    "            for doc, out in self.data:\n",
    "                self.lines.append([])\n",
    "                doc = doc[:max_doc_len]\n",
    "                for line in doc:\n",
    "                    self.lines[-1].append([0 for _ in range(max_sen_len - len(line))] + line[:max_sen_len])\n",
    "                self.output.append(out[:max_doc_len])\n",
    "                \n",
    "    def padded_dump(self, filename):\n",
    "        pickle.dump([self.lines, self.output], open(filename, 'wb+'))\n",
    "        \n",
    "    def load_padded_dump(self, filename):\n",
    "        self.lines, self.output = pickle.load(open(filename, 'rb+'))\n",
    "        self.output = np.array(self.output)\n",
    "        self.output[self.output == 2] = 0\n",
    "        \n",
    "    def make_batches(self, batch_size):\n",
    "        self.lengths = [[] for _ in range(90)]\n",
    "        self.batches = []\n",
    "        for i in range(len(self.lines)):\n",
    "            self.lengths[len(self.lines[i]) -1].append(i)\n",
    "        for i in self.lengths:\n",
    "            for j in range(0, len(i), batch_size):\n",
    "                self.batches.append(i[j:j+batch_size])\n",
    "        random.shuffle(self.batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refresh",
   "language": "python",
   "name": "refresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
