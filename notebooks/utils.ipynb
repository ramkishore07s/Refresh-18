{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSIFIER UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(model, lines, output, batches, iterations=1):\n",
    "    '''\n",
    "    model: Pytorch class instance which inherits nn.Module\n",
    "    lines: list of documents, each document is a list of list of tokens' indices\n",
    "    output: binary labels for each sentence\n",
    "    '''\n",
    "    errors = []\n",
    "    lossfunc = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for _ in range(iterations):\n",
    "        for batch in batches:\n",
    "            try:\n",
    "                if len(batch) == 20:\n",
    "                    input = torch.cuda.LongTensor([lines[n] for n in batch])\n",
    "                    truth = torch.cuda.FloatTensor([output[n] for n in batch])\n",
    "                    truth[truth == 2] = 0\n",
    "                    pred = model(input)\n",
    "                    model.zero_grad()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = lossfunc(pred, truth)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    errors.append(loss.data)\n",
    "            except:\n",
    "                print('ad')\n",
    "            print(batches.index(batch), end='\\r')\n",
    "        plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([0.1, 0, 2, 3]).ge(0.5).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summaries(model, lines_, batches, doc_folder, write_folder, output_dim=1, summary_len='variable'):\n",
    "    '''\n",
    "    model: Pytorch class instance which inherits nn.Module\n",
    "    lines_: list of documents, each document is a list of list of tokens' indices\n",
    "    batches: indices of documents grouped by batches\n",
    "    write_folder: folder to which to write the summary to\n",
    "    output_dim: no. of outputs given by the model for each sentence, can only be 1 or 2\n",
    "    summary_len: no. of lines to extract as summary. a number or string 'variable'\n",
    "    '''\n",
    "    for batch in batches:\n",
    "        input = torch.cuda.LongTensor([lines_[n] for n in batch])\n",
    "        pred = model(input)\n",
    "\n",
    "        if output_dim == 1:\n",
    "            _idx = torch.sort(pred, descending=True)[1].data\n",
    "            for lines, docid in zip(_idx, batch):\n",
    "                with open(doc_folder + str(docid)) as f:\n",
    "                    content = f.readlines()\n",
    "                    selected_lines = [content[l] for l in lines[0:3]]\n",
    "        else:\n",
    "            for lines, docid in zip(pred.data, batch):\n",
    "                with open(doc_folder + str(docid)) as f:\n",
    "                    content = f.readlines()\n",
    "                    #print([l[1] for l in lines])\n",
    "                    #selected_lines = [content[i] for i, l in zip(range(len(lines)), lines) if l[1].ge(0.5)]\n",
    "                    pos = [l[1].data for l in lines]\n",
    "                    neg = [l[0].data for l in lines]\n",
    "                    if summary_len == 'variable':\n",
    "                        selected_lines = []\n",
    "                        for i in range(len(pos)):\n",
    "                            if pos[i] > neg[i]:\n",
    "                                selected_lines.append(i)\n",
    "                    else:\n",
    "                        selected_lines = list(zip(*sorted(zip(pos, range(len(pos))), key=lambda x: x[0], reverse=True)))[1][0:summary_len]\n",
    "                    #print(selected_lines)\n",
    "                    #print(len(content))\n",
    "                    #print(docid, len(content), selected_lines, batch)\n",
    "                    selected_lines = [content[s] for s in selected_lines if s < len(content) ]\n",
    "            \n",
    "                with open(write_folder + str(docid), 'w+') as f2:\n",
    "                    [f2.write(line) for line in selected_lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file(doc_id, write_folder):\n",
    "    print(open(write_folder + str(doc_id)).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracies(model, lines_, output, batches):\n",
    "    '''\n",
    "    computes precision, recall, f1 on output labels\n",
    "    '''\n",
    "    tp, fn, fp = 0, 0, 0\n",
    "    for batch in batches:\n",
    "        input = torch.cuda.LongTensor([lines_[n] for n in batch])\n",
    "        truth = torch.cuda.ByteTensor([output[n] for n in batch])\n",
    "        pred = model(input)\n",
    "        tp += torch.sum(pred.gt(0.5) * truth)\n",
    "        fn += torch.sum(pred.le(0.5) * truth)\n",
    "        fp += torch.sum(pred.gt(0.5) * truth.le(0.))\n",
    "    tp = tp.float()\n",
    "    fn = fn.float()\n",
    "    fp = fp.float()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall /(precision + recall)\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall ,'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REFRESH UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_reinforce_loss(bceloss, pred_, scores_, max_=5):\n",
    "    '''\n",
    "    TODO: probably same as compute_refresh_loss_avg_sample, remove it\n",
    "    '''\n",
    "    l = Variable(torch.zeros(1), requires_grad=True).cuda()\n",
    "    for pred, scores in zip(pred_, scores_):\n",
    "        for sentences, score in scores[0:max_]:\n",
    "            truth = np.zeros(pred.size(0))\n",
    "            truth[list(sentences)] = 1\n",
    "            truth = torch.cuda.FloatTensor(truth)\n",
    "            score = torch.cuda.FloatTensor([score])[0]\n",
    "            l = l + bceloss(pred, truth) * score\n",
    "    return l / (20. * max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_reinforce(model, lines, scores, batches, iterations, max_=5):\n",
    "    errors = []\n",
    "    lossfunc = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for _ in range(iterations):\n",
    "        for batch in batches:\n",
    "            if len(batch) == 20:\n",
    "                optimizer.zero_grad()\n",
    "                input = torch.cuda.LongTensor([lines[n] for n in batch])\n",
    "                scores_ = [scores[i] for i in batch]\n",
    "                pred = model(input)\n",
    "\n",
    "                loss = compute_reinforce_loss(lossfunc, pred, scores_, max_)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                errors.append(loss.data)\n",
    "\n",
    "\n",
    "\n",
    "                print(loss.data, batches.index(batch), end='\\r')\n",
    "        plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_refresh_loss_avg_sample(lossfunc, pred_, scores_, max_):\n",
    "    l = Variable(torch.zeros(1), requires_grad=True).cuda()\n",
    "    for pred, scores in zip(pred_, scores_):\n",
    "        for sentences, score in scores[0:max_]:\n",
    "            truth = np.zeros(pred.size(0))\n",
    "            truth[list(sentences)] = 1\n",
    "            truth = torch.cuda.LongTensor(truth)\n",
    "            score = torch.cuda.FloatTensor([score])[0]\n",
    "            l = l + lossfunc(pred, truth) * score\n",
    "    return l / (20. * max_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_refresh_loss_single_sample(lossfunc, pred_, scores_, max_):\n",
    "    l = Variable(torch.zeros(1), requires_grad=True).cuda()\n",
    "    for pred, scores in zip(pred_, scores_):\n",
    "        randint = random.randint(0, min(max_ , len(scores) - 1))\n",
    "        #print('\\r', randint, end='')\n",
    "        sentences, score = scores[randint]\n",
    "        truth = np.zeros(pred.size(0))\n",
    "        truth[list(sentences)] = 1\n",
    "        truth = torch.cuda.LongTensor(truth)\n",
    "        score = torch.cuda.FloatTensor([score])[0]\n",
    "        l = l + lossfunc(pred, truth) * score\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_refresh(model, lines, scores, batches, iterations, max_=5, single_sample=True):\n",
    "    errors = []\n",
    "    lossfunc = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for _ in range(iterations):\n",
    "        for batch in batches:\n",
    "            if len(batch) == 20:\n",
    "                model.zero_grad()\n",
    "\n",
    "                input = torch.cuda.LongTensor([lines[n] for n in batch])\n",
    "                scores_ = [scores[i] for i in batch]\n",
    "                pred = model(input)\n",
    "                if single_sample:\n",
    "                    loss = compute_refresh_loss_single_sample(lossfunc, pred, scores_, max_)\n",
    "                else:\n",
    "                    loss = compute_refresh_loss_avg_sample(lossfunc, pred, scores_, max_)\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "                errors.append(loss.data)\n",
    "\n",
    "                print('\\r','training: ', loss.data, batches.index(batch), end='')\n",
    "      #  plt.plot(errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
